{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"uPGNGe7xWyl1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1733672421813,"user_tz":-210,"elapsed":15130,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"outputId":"6360c805-6afc-4f98-bf23-4f6d2fbf6b9f"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m471.0/480.6 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/179.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/134.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/194.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}],"source":["!pip install transformers datasets torch  --q"]},{"cell_type":"code","source":["!pip install SpeechRecognition\n","!pip install ffmpeg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w4zcIl98uelq","executionInfo":{"status":"ok","timestamp":1733672725209,"user_tz":-210,"elapsed":9699,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"outputId":"83256414-35aa-432b-e861-7365d8ef43e0"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: SpeechRecognition in /usr/local/lib/python3.10/dist-packages (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from SpeechRecognition) (4.12.2)\n","Requirement already satisfied: ffmpeg in /usr/local/lib/python3.10/dist-packages (1.4)\n"]}]},{"cell_type":"code","source":["import subprocess\n","\n","result = subprocess.run(['ffmpeg', '-version'], capture_output=True, text=True)\n","print(result.stdout)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"idjfdcuHWaHX","executionInfo":{"status":"ok","timestamp":1733672428578,"user_tz":-210,"elapsed":1237,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"outputId":"fa4e3954-4671-4fa5-a3a4-b8bbe3bc7cd5"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n","built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n","configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n","libavutil      56. 70.100 / 56. 70.100\n","libavcodec     58.134.100 / 58.134.100\n","libavformat    58. 76.100 / 58. 76.100\n","libavdevice    58. 13.100 / 58. 13.100\n","libavfilter     7.110.100 /  7.110.100\n","libswscale      5.  9.100 /  5.  9.100\n","libswresample   3.  9.100 /  3.  9.100\n","libpostproc    55.  9.100 / 55.  9.100\n","\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u66y01HgXabD","executionInfo":{"status":"ok","timestamp":1733672770399,"user_tz":-210,"elapsed":22452,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"outputId":"f38a1912-0026-4cc2-f1ad-8ce16f81d060"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import os\n","\n","# path = \"drive/MyDrive/Speech2Text/dataset/madar0/\"\n","path = \"drive/MyDrive/Speech2Text/dataset/\"\n","\n","if not os.path.exists(path):\n","    try:\n","        os.makedirs(path)\n","        print(f\"The directory '{path}' has been created successfully.\")\n","    except OSError as e:\n","        print(f\"Error creating directory: {e}\")\n","else:\n","    print(f\"The directory '{path}' already exists.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mIHi889hXLzP","executionInfo":{"status":"ok","timestamp":1733672791615,"user_tz":-210,"elapsed":551,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"outputId":"3fda2395-a1da-4c8c-bc91-14b8a93da560"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["The directory 'drive/MyDrive/Speech2Text/dataset/' already exists.\n"]}]},{"cell_type":"code","source":["import subprocess\n","\n","# convert mp3 to wav file\n","subprocess.call(['ffmpeg', '-i', f\"{path}/grandfather. 2015-10-09_20'20'35'.mp3\",\n","                 f'{path}/converted_to_wav_file.wav'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZSW7DY6y8nZp","executionInfo":{"status":"ok","timestamp":1733672877390,"user_tz":-210,"elapsed":425,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"outputId":"5a6025a9-f6c2-4b21-b4a6-0c42d62c3680"},"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","source":["!cd /content/drive/\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uB_SiimEW2Xk","executionInfo":{"status":"ok","timestamp":1733400159225,"user_tz":-210,"elapsed":7,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"outputId":"da9d3e3e-4c0d-4c5e-ed5f-cc2fda9cbdae"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["!ffmpeg -i /content/drive/MyDrive/Speech2Text/dataset/madar0/Lab41-SRI-VOiCES-madar.wav -ar 16000 -ac 1 output.wav"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kqnk9Sfkuy74","executionInfo":{"status":"ok","timestamp":1733400182300,"user_tz":-210,"elapsed":23078,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"outputId":"3b015c59-e3e7-4f6d-eb17-00531e251fcd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["ffmpeg version 4.4.2-0ubuntu0.22.04.1 Copyright (c) 2000-2021 the FFmpeg developers\n","  built with gcc 11 (Ubuntu 11.2.0-19ubuntu1)\n","  configuration: --prefix=/usr --extra-version=0ubuntu0.22.04.1 --toolchain=hardened --libdir=/usr/lib/x86_64-linux-gnu --incdir=/usr/include/x86_64-linux-gnu --arch=amd64 --enable-gpl --disable-stripping --enable-gnutls --enable-ladspa --enable-libaom --enable-libass --enable-libbluray --enable-libbs2b --enable-libcaca --enable-libcdio --enable-libcodec2 --enable-libdav1d --enable-libflite --enable-libfontconfig --enable-libfreetype --enable-libfribidi --enable-libgme --enable-libgsm --enable-libjack --enable-libmp3lame --enable-libmysofa --enable-libopenjpeg --enable-libopenmpt --enable-libopus --enable-libpulse --enable-librabbitmq --enable-librubberband --enable-libshine --enable-libsnappy --enable-libsoxr --enable-libspeex --enable-libsrt --enable-libssh --enable-libtheora --enable-libtwolame --enable-libvidstab --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx265 --enable-libxml2 --enable-libxvid --enable-libzimg --enable-libzmq --enable-libzvbi --enable-lv2 --enable-omx --enable-openal --enable-opencl --enable-opengl --enable-sdl2 --enable-pocketsphinx --enable-librsvg --enable-libmfx --enable-libdc1394 --enable-libdrm --enable-libiec61883 --enable-chromaprint --enable-frei0r --enable-libx264 --enable-shared\n","  libavutil      56. 70.100 / 56. 70.100\n","  libavcodec     58.134.100 / 58.134.100\n","  libavformat    58. 76.100 / 58. 76.100\n","  libavdevice    58. 13.100 / 58. 13.100\n","  libavfilter     7.110.100 /  7.110.100\n","  libswscale      5.  9.100 /  5.  9.100\n","  libswresample   3.  9.100 /  3.  9.100\n","  libpostproc    55.  9.100 / 55.  9.100\n","\u001b[0;33mGuessed Channel Layout for Input Stream #0.0 : stereo\n","\u001b[0mInput #0, wav, from '/content/drive/MyDrive/Speech2Text/dataset/madar0/Lab41-SRI-VOiCES-madar.wav':\n","  Metadata:\n","    artist          :  @monadchannel\n","    title           : مدار صفر درجه؛ علی‌‌رضا قربانی. \n","    encoder         : Lavf60.3.100\n","  Duration: 00:03:48.48, bitrate: 1411 kb/s\n","  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, stereo, s16, 1411 kb/s\n","File 'output.wav' already exists. Overwrite? [y/N] n\n","\u001b[4;31mNot overwriting - exiting\n","\u001b[0m"]}]},{"cell_type":"code","source":["import speech_recognition as sr\n","import logging\n","\n","logging.basicConfig(level=logging.DEBUG)\n","\n","def transcribe_audio(file_path):\n","    \"\"\"\n","    Transcribes an audio file in WAV format to Persian text.\n","\n","    Parameters:\n","    file_path (str): Path to the WAV audio file.\n","\n","    Returns:\n","    str: The transcribed text or an error message.\n","    \"\"\"\n","    recognizer = sr.Recognizer()\n","\n","    try:\n","        with sr.AudioFile(file_path) as source:\n","            print(\"Loading audio...\")\n","            audio_data = recognizer.record(source)\n","\n","        print(\"Transcribing audio...\")\n","        transcription = recognizer.recognize_google(audio_data, language=\"fa-IR\")\n","        return transcription\n","\n","    except FileNotFoundError:\n","        return \"Error: File not found. Please provide a valid file path.\"\n","    except sr.UnknownValueError:\n","        return \"Error: Speech could not be understood.\"\n","    except sr.RequestError as e:\n","        return f\"Error: Could not request results; {e}\"\n","    except Exception as e:\n","        return f\"Unexpected error: {e}\"\n","\n","if __name__ == \"__main__\":\n","    main_path = '/content/drive/MyDrive/Speech2Text/dataset/madar0'\n","    audio_file = f\"{main_path}/output.wav\"\n","    result = transcribe_audio(audio_file)\n","\n","    output_file = f\"{main_path}/transcription_output_whole_duration.txt\"\n","    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n","        file.write(result)\n","\n","    print(\"Transcription:\", result)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cun4BBTXuWrW","executionInfo":{"status":"ok","timestamp":1733400201375,"user_tz":-210,"elapsed":19080,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"outputId":"01a2cd10-dcf7-429f-ac73-d14b59a886ce"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading audio...\n","Transcribing audio...\n","Transcription: وقتی گریبانم با دست خلقت می‌درید وقتی ابد چشم تو را پیش از ازل می‌افتد تو را در آسمان‌ها می‌کشید وقتی آتش تو را با اشکایم می‌کشید من عاشق چشم شدم نه عقل بود و نه دلی چیزی نمی‌دانم از این دیوانگی و آواز یک آن شد این عاشق شدن دنیا همان یک لحظه بودم که چشمانش مرا از عمق چشمان وقتی که من عاشق شدم شیطان به نامت سجده کرد آدم زمینی تر شد و عالم به آدم سجده کرد من بودم و چشمان تو آتشی و نه گلی چیزی نمی‌دانم از این دیوانگی و عاقلی من عاشق چشم شدم شاید کمی هم بیشتر چیزی در آن سوی یقین شاید کمی هم کیشتر آغاز و ختم ماجرا لمس تماشا تو بود دیگر فقط تصویر من در مردمک های تو بود دیوانه\n"]}]},{"cell_type":"code","source":["import speech_recognition as sr\n","import wave\n","import math\n","\n","def get_audio_duration(file_path):\n","    \"\"\"\n","    Calculate the duration of a WAV file in seconds.\n","\n","    Parameters:\n","    file_path (str): Path to the WAV audio file.\n","\n","    Returns:\n","    float: Duration of the audio file in seconds.\n","    \"\"\"\n","    with wave.open(file_path, \"r\") as audio_file:\n","        frames = audio_file.getnframes()\n","        rate = audio_file.getframerate()\n","        return frames / float(rate)\n","\n","def transcribe_audio_in_chunks(file_path, output_file, chunk_duration=20, overlap=10):\n","    \"\"\"\n","    Transcribes a long audio file in WAV format to Persian text by processing it in chunks.\n","    Saves each chunk's transcription in a new line in the output file.\n","\n","    Parameters:\n","    file_path (str): Path to the WAV audio file.\n","    output_file (str): Path to the output text file.\n","    chunk_duration (int): Duration of each audio chunk in seconds.\n","    overlap (int): Overlap duration between consecutive chunks in seconds.\n","    \"\"\"\n","    recognizer = sr.Recognizer()\n","\n","    try:\n","        audio_duration = get_audio_duration(file_path)\n","        print(f\"Audio duration: {audio_duration:.2f} seconds\")\n","\n","        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n","            # Split audio into overlapping chunks\n","            with sr.AudioFile(file_path) as source:\n","                start = 0\n","                while start < audio_duration:\n","                    end = min(start + chunk_duration, audio_duration)\n","                    print(f\"Processing chunk: {start} to {end} seconds\")\n","\n","                    # Adjust duration for the last chunk\n","                    audio_data = recognizer.record(source, duration=(end - start))\n","\n","                    try:\n","                        # Transcribe the chunk\n","                        chunk_transcription = recognizer.recognize_google(audio_data, language=\"fa-IR\")\n","                        file.write(chunk_transcription + \"\\n\")\n","                        print(f\"Chunk {start}-{end}: {chunk_transcription}\")\n","\n","                    except sr.UnknownValueError:\n","                        file.write(\"[Unintelligible]\\n\")\n","                        print(f\"Chunk {start}-{end}: [Unintelligible]\")\n","                    except sr.RequestError as e:\n","                        file.write(\"[Error in chunk]\\n\")\n","                        print(f\"Error in chunk {start}-{end}: {e}\")\n","\n","                    # Move to the next chunk with overlap\n","                    start = start + chunk_duration - overlap\n","\n","    except FileNotFoundError:\n","        print(\"Error: File not found. Please provide a valid file path.\")\n","    except Exception as e:\n","        print(f\"Unexpected error: {e}\")\n","\n","if __name__ == \"__main__\":\n","    main_path = '/content/drive/MyDrive/Speech2Text/dataset'\n","    audio_file = f\"{main_path}/converted_to_wav_file.wav\"\n","    output_file = f\"{main_path}/transcription_output_duration45.txt\"\n","    transcribe_audio_in_chunks(audio_file, output_file, chunk_duration=45, overlap=15)\n","    print(f\"Transcription saved to {output_file}\")"],"metadata":{"collapsed":true,"id":"2yEghBJNv7l5","executionInfo":{"status":"ok","timestamp":1733672971081,"user_tz":-210,"elapsed":44545,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"624ee73c-563a-4723-c6e3-19cb069d5880"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Audio duration: 679.00 seconds\n","Processing chunk: 0 to 45 seconds\n","Chunk 0-45: داشتم داشتم مامان مامان گفته بودن مادر بزرگ\n","Processing chunk: 30 to 75 seconds\n","Chunk 30-75: [Unintelligible]\n","Processing chunk: 60 to 105 seconds\n","Chunk 60-105: خیلی چقدر\n","Processing chunk: 90 to 135 seconds\n","Chunk 90-135: دختراشون جوونی بودن که فوت شدن گفتن سر فوت دیگه آقا مثلاً چشمشون دیگه کور شد انقدر مامان می‌گفتن سر فوت آقا دیگه چشمشون کور شد آره نه مامان مادر\n","Processing chunk: 120 to 165 seconds\n","Chunk 120-165: دختر\n","Processing chunk: 150 to 195 seconds\n","Chunk 150-195: کسی یه کاری میکرد که ناجور بود\n","Processing chunk: 180 to 225 seconds\n","Chunk 180-225: خیلی مغرور بودن\n","Processing chunk: 210 to 255 seconds\n","Chunk 210-255: اذیت کنی بودن نه\n","Processing chunk: 240 to 285 seconds\n","Chunk 240-285: خودشونو داشتن بعد اینا نه مثلاً اینا اصلاً اینجوری احترام عمه رو نگه نمیدداشت احترام بله عمه مامان دیگه خاله\n","Processing chunk: 270 to 315 seconds\n","Chunk 270-315: پسر کی کی من دیگه\n","Processing chunk: 300 to 345 seconds\n","Chunk 300-345: پدر پدر بزرگ ایشان پدر\n","Processing chunk: 330 to 375 seconds\n","Chunk 330-375: سلام بله بله\n","Processing chunk: 360 to 405 seconds\n","Chunk 360-405: کاغذ بنویسم کاغذ نوشت پشت سر مسافر\n","Processing chunk: 390 to 435 seconds\n","Chunk 390-435: منظور کتاب خیلی حرفهای صحیح هم داره\n","Processing chunk: 420 to 465 seconds\n","Chunk 420-465: زیاد دارند تو جنده از اینجور چیزا زیاده نمی‌دونم چهارشنبه این کار نکن شنبه اون کار نکنی از اینجور چیزاست دیگه\n","Processing chunk: 450 to 495 seconds\n","Chunk 450-495: [Unintelligible]\n","Processing chunk: 480 to 525 seconds\n","Chunk 480-525: [Unintelligible]\n","Processing chunk: 510 to 555 seconds\n","Chunk 510-555: [Unintelligible]\n","Processing chunk: 540 to 585 seconds\n","Chunk 540-585: [Unintelligible]\n","Processing chunk: 570 to 615 seconds\n","Chunk 570-615: [Unintelligible]\n","Processing chunk: 600 to 645 seconds\n","Chunk 600-645: [Unintelligible]\n","Processing chunk: 630 to 675 seconds\n","Chunk 630-675: [Unintelligible]\n","Processing chunk: 660 to 679.0008163265306 seconds\n","Chunk 660-679.0008163265306: [Unintelligible]\n","Transcription saved to /content/drive/MyDrive/Speech2Text/dataset/transcription_output_duration45.txt\n"]}]},{"cell_type":"code","source":["import os\n","import wave\n","import math\n","import subprocess\n","import speech_recognition as sr\n","\n","\n","def preprocess_audio(input_file, output_file):\n","    \"\"\"\n","    Preprocess audio file: convert to mono, normalize, and reduce noise.\n","    \"\"\"\n","    print(\"Preprocessing audio...\")\n","\n","    subprocess.run([\n","        \"ffmpeg\", \"-y\", \"-i\", input_file, \"-ac\", \"1\", \"-ar\", \"16000\", output_file\n","    ])\n","    print(\"Preprocessing completed. File saved as:\", output_file)\n","\n","\n","def get_audio_duration(file_path):\n","    \"\"\"\n","    Calculate the duration of a WAV file in seconds.\n","    \"\"\"\n","    with wave.open(file_path, \"rb\") as audio_file:\n","        frames = audio_file.getnframes()\n","        rate = audio_file.getframerate()\n","        return frames / float(rate)\n","\n","\n","def transcribe_audio_in_chunks(file_path, output_file, chunk_duration=20, overlap=10):\n","    \"\"\"\n","    Transcribes a long audio file in WAV format to Persian text by processing it in chunks.\n","    Saves each chunk's transcription in a new line in the output file.\n","\n","    Parameters:\n","    file_path (str): Path to the WAV audio file.\n","    output_file (str): Path to the output text file.\n","    chunk_duration (int): Duration of each audio chunk in seconds.\n","    overlap (int): Overlap duration between consecutive chunks in seconds.\n","    \"\"\"\n","    recognizer = sr.Recognizer()\n","\n","    try:\n","        audio_duration = get_audio_duration(file_path)\n","        print(f\"Audio duration: {audio_duration:.2f} seconds\")\n","\n","        # Open the output file for writing\n","        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n","            start = 0\n","\n","            with wave.open(file_path, \"rb\") as audio:\n","                framerate = audio.getframerate()\n","                while start < audio_duration:\n","                    end = min(start + chunk_duration, audio_duration)\n","\n","                    # Calculate start and end frames\n","                    start_frame = int(start * framerate)\n","                    end_frame = int(end * framerate)\n","\n","                    # Read frames for the current chunk\n","                    audio.setpos(start_frame)\n","                    chunk_data = audio.readframes(end_frame - start_frame)\n","\n","                    # Write chunk data to a temporary WAV file\n","                    with wave.open(\"temp_chunk.wav\", \"wb\") as temp_audio:\n","                        temp_audio.setnchannels(audio.getnchannels())\n","                        temp_audio.setsampwidth(audio.getsampwidth())\n","                        temp_audio.setframerate(framerate)\n","                        temp_audio.writeframes(chunk_data)\n","\n","                    # Transcribe the chunk\n","                    with sr.AudioFile(\"temp_chunk.wav\") as source:\n","                        audio_data = recognizer.record(source)\n","                        try:\n","                            transcription = recognizer.recognize_google(audio_data, language=\"fa-IR\")\n","                            file.write(transcription + \"\\n\")\n","                            print(f\"Chunk {start:.2f}-{end:.2f} transcribed: {transcription}\")\n","                        except sr.UnknownValueError:\n","                            file.write(\"[Unintelligible]\\n\")\n","                            print(f\"Chunk {start:.2f}-{end:.2f}: [Unintelligible]\")\n","                        except sr.RequestError as e:\n","                            file.write(\"[Error in chunk]\\n\")\n","                            print(f\"Error in chunk {start:.2f}-{end:.2f}: {e}\")\n","\n","                    # Move to the next chunk with overlap\n","                    start = start + chunk_duration - overlap\n","\n","    except FileNotFoundError:\n","        print(\"Error: File not found. Please provide a valid file path.\")\n","    except Exception as e:\n","        print(f\"Unexpected error: {e}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main_path = '/content/drive/MyDrive/Speech2Text/dataset'\n","    audio_file = f\"{main_path}/converted_to_wav_file.wav\"\n","    output_file = f\"{main_path}/transcription_output.txt\"\n","\n","    preprocessed_audio = f\"{main_path}/preprocessed.wav\"\n","    transcription_output = f\"{main_path}/transcription_output_chunks_20_with_overlap.txt\"\n","\n","    preprocess_audio(audio_file, preprocessed_audio)\n","    transcribe_audio_in_chunks(preprocessed_audio, transcription_output, chunk_duration=20, overlap=5)\n","\n","    print(f\"Transcription saved to {transcription_output}\")"],"metadata":{"id":"gFoAimtx0_O0","executionInfo":{"status":"ok","timestamp":1733673150157,"user_tz":-210,"elapsed":59221,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6a15c118-f07e-430b-8bc5-f45fff999243"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessing audio...\n","Preprocessing completed. File saved as: /content/drive/MyDrive/Speech2Text/dataset/preprocessed.wav\n","Audio duration: 679.00 seconds\n","Chunk 0.00-20.00 transcribed: اونجا بودن\n","Chunk 15.00-35.00: [Unintelligible]\n","Chunk 30.00-50.00: [Unintelligible]\n","Chunk 45.00-65.00: [Unintelligible]\n","Chunk 60.00-80.00: [Unintelligible]\n","Chunk 75.00-95.00: [Unintelligible]\n","Chunk 90.00-110.00: [Unintelligible]\n","Chunk 105.00-125.00: [Unintelligible]\n","Chunk 120.00-140.00 transcribed: نورانی بود چقدر داغ مادربزرگ\n","Chunk 135.00-155.00 transcribed: ۴ تا خب هم دوتا پدربزرگان هم دوتا از دختراشون جوونی بودن که فوت شدن گفتن چشمشون دیگه کور شد انقدر مامان می‌گفتن سر فوت آقا دیگه چشمشون کور شد\n","Chunk 150.00-170.00 transcribed: بچه\n","Chunk 165.00-185.00 transcribed: نمیدونم\n","Chunk 180.00-200.00 transcribed: مادربزرگ مامان جون می‌شه\n","Chunk 195.00-215.00 transcribed: خدمت\n","Chunk 210.00-230.00: [Unintelligible]\n","Chunk 225.00-245.00: [Unintelligible]\n","Chunk 240.00-260.00 transcribed: چرا\n","Chunk 255.00-275.00 transcribed: تذکر می‌دادن\n","Chunk 270.00-290.00 transcribed: مغرور بودن خیلی مغرور بودن\n","Chunk 285.00-305.00 transcribed: خیلی مغرور بودن\n","Chunk 300.00-320.00: [Unintelligible]\n","Chunk 315.00-335.00 transcribed: یکیشون میومدم خونه بازی تهران بودن زندگی\n","Chunk 330.00-350.00 transcribed: ضبط کرده\n","Chunk 345.00-365.00 transcribed: اذیت کنی بودن نه عروساشون اذیتشون می‌کردن خیلی\n","Chunk 360.00-380.00 transcribed: عروس‌هاشون اذیتشون می‌کردن خیلی\n","Chunk 375.00-395.00 transcribed: عمه دیسیپلین خودشونو داشتن بعد اینا نه مثلاً اینا اصلاً اینجوری احترام عمه رو نگه نمی‌داشتند مامان می‌گفتن احترامشونو خیلی نگه نمی‌داشتن عروسا شو بله\n","Chunk 390.00-410.00 transcribed: عروساشو بله\n","Chunk 405.00-425.00 transcribed: در حال\n","Chunk 420.00-440.00 transcribed: در حال اطاعت تا کتاب پسر\n","Chunk 435.00-455.00 transcribed: پسر کی پدربزرگای من دیگه\n","Chunk 450.00-470.00 transcribed: بابا میگه پدراشون یکیه\n","Chunk 465.00-485.00 transcribed: پدراشون یکیه پدر پدر بزرگ\n","Chunk 480.00-500.00 transcribed: پدر پدر\n","Chunk 495.00-515.00 transcribed: چیکاره بودن\n","Chunk 510.00-530.00: [Unintelligible]\n","Chunk 525.00-545.00: [Unintelligible]\n","Chunk 540.00-560.00: [Unintelligible]\n","Chunk 555.00-575.00 transcribed: کاغذ سفیدم یه کاغذ سفید\n","Chunk 570.00-590.00 transcribed: خوب نه\n","Chunk 585.00-605.00 transcribed: کتاب یک کتاب است راجع به آداب\n","Chunk 600.00-620.00 transcribed: یک کتاب است راجع به آداب معاشرت با زنان منتها نه اینا یه سری خرافات خیلی هم داره\n","Chunk 615.00-635.00 transcribed: خیلی حرف‌های صحیحی هم داره\n","Chunk 630.00-650.00 transcribed: زیاد دارند از اینجور چیزا زیاده نمی‌دونم چهارشنبه این کار نکن شنبه اون کار نکنی از اینجور چیزاست دیگه\n","Chunk 645.00-665.00 transcribed: چشم\n","Chunk 660.00-679.00: [Unintelligible]\n","Chunk 675.00-679.00: [Unintelligible]\n","Transcription saved to /content/drive/MyDrive/Speech2Text/dataset/transcription_output_chunks_20_with_overlap.txt\n"]}]},{"cell_type":"markdown","source":["## Updated Script with Deduplication\n"],"metadata":{"id":"D3_Vm_1K4eJg"}},{"cell_type":"code","source":["import os\n","import wave\n","import math\n","import subprocess\n","import speech_recognition as sr\n","from difflib import SequenceMatcher\n","\n","\n","def preprocess_audio(input_file, output_file):\n","    \"\"\"\n","    Preprocess audio file: convert to mono, normalize, and reduce noise.\n","    \"\"\"\n","    print(\"Preprocessing audio...\")\n","    subprocess.run([\n","        \"ffmpeg\", \"-y\", \"-i\", input_file, \"-ac\", \"1\", \"-ar\", \"16000\", output_file\n","    ])\n","    print(\"Preprocessing completed. File saved as:\", output_file)\n","\n","\n","def get_audio_duration(file_path):\n","    \"\"\"\n","    Calculate the duration of a WAV file in seconds.\n","    \"\"\"\n","    with wave.open(file_path, \"rb\") as audio_file:\n","        frames = audio_file.getnframes()\n","        rate = audio_file.getframerate()\n","        return frames / float(rate)\n","\n","\n","def deduplicate_text(current_text, previous_text):\n","    \"\"\"\n","    Deduplicate overlapping text by comparing the current chunk with the previous chunk.\n","\n","    Parameters:\n","    current_text (str): Text from the current chunk.\n","    previous_text (str): Text from the previous chunk.\n","\n","    Returns:\n","    str: Deduplicated text.\n","    \"\"\"\n","    if not previous_text:\n","        return current_text\n","\n","    # Compare overlapping text using SequenceMatcher\n","    overlap_start = max(0, len(previous_text) - 50)\n","    previous_overlap = previous_text[overlap_start:]\n","    match = SequenceMatcher(None, previous_overlap, current_text).find_longest_match(\n","        0, len(previous_overlap), 0, len(current_text)\n","    )\n","\n","    # If overlap found, remove it from the current chunk\n","    if match.size > 0:\n","        start_index = match.b + match.size\n","        return current_text[start_index:]\n","\n","    return current_text\n","\n","\n","def transcribe_audio_in_chunks(file_path, output_file, chunk_duration=20, overlap=10):\n","    \"\"\"\n","    Transcribes a long audio file in WAV format to Persian text by processing it in chunks.\n","    Deduplicates overlapping text and saves each chunk's transcription in a new line.\n","\n","    Parameters:\n","    file_path (str): Path to the WAV audio file.\n","    output_file (str): Path to the output text file.\n","    chunk_duration (int): Duration of each audio chunk in seconds.\n","    overlap (int): Overlap duration between consecutive chunks in seconds.\n","    \"\"\"\n","    recognizer = sr.Recognizer()\n","    previous_transcription = \"\"\n","\n","    try:\n","        audio_duration = get_audio_duration(file_path)\n","        print(f\"Audio duration: {audio_duration:.2f} seconds\")\n","\n","        # Open the output file for writing\n","        with open(output_file, \"w\", encoding=\"utf-8\") as file:\n","            start = 0\n","\n","            with wave.open(file_path, \"rb\") as audio:\n","                framerate = audio.getframerate()\n","                while start < audio_duration:\n","                    end = min(start + chunk_duration, audio_duration)\n","\n","                    # Calculate start and end frames\n","                    start_frame = int(start * framerate)\n","                    end_frame = int(end * framerate)\n","\n","                    # Read frames for the current chunk\n","                    audio.setpos(start_frame)\n","                    chunk_data = audio.readframes(end_frame - start_frame)\n","\n","                    # Write chunk data to a temporary WAV file\n","                    with wave.open(\"temp_chunk.wav\", \"wb\") as temp_audio:\n","                        temp_audio.setnchannels(audio.getnchannels())\n","                        temp_audio.setsampwidth(audio.getsampwidth())\n","                        temp_audio.setframerate(framerate)\n","                        temp_audio.writeframes(chunk_data)\n","\n","                    # Transcribe the chunk\n","                    with sr.AudioFile(\"temp_chunk.wav\") as source:\n","                        audio_data = recognizer.record(source)\n","                        try:\n","                            transcription = recognizer.recognize_google(audio_data, language=\"fa-IR\")\n","\n","                            # Deduplicate overlapping text\n","                            transcription = deduplicate_text(transcription, previous_transcription)\n","                            previous_transcription = transcription\n","\n","                            # Save deduplicated transcription\n","                            file.write(transcription + \"\\n\")\n","                            print(f\"Chunk {start:.2f}-{end:.2f} transcribed: {transcription}\")\n","                        except sr.UnknownValueError:\n","                            file.write(\"[Unintelligible]\\n\")\n","                            print(f\"Chunk {start:.2f}-{end:.2f}: [Unintelligible]\")\n","                        except sr.RequestError as e:\n","                            file.write(\"[Error in chunk]\\n\")\n","                            print(f\"Error in chunk {start:.2f}-{end:.2f}: {e}\")\n","\n","                    # Move to the next chunk with overlap\n","                    start = start + chunk_duration - overlap\n","\n","    except FileNotFoundError:\n","        print(\"Error: File not found. Please provide a valid file path.\")\n","    except Exception as e:\n","        print(f\"Unexpected error: {e}\")\n","\n","\n","if __name__ == \"__main__\":\n","    main_path = '/content/drive/MyDrive/Speech2Text/dataset/'\n","    original_audio = f\"{main_path}/converted_to_wav_file.wav\"\n","    preprocessed_audio = f\"{main_path}/preprocessed.wav\"\n","    transcription_output = f\"{main_path}/transcription_output_chunk_30_cover_overlap.txt\"\n","\n","    preprocess_audio(original_audio, preprocessed_audio)\n","\n","    transcribe_audio_in_chunks(preprocessed_audio, transcription_output, chunk_duration=30, overlap=5)\n","\n","    print(f\"Transcription saved to {transcription_output}\")"],"metadata":{"id":"fkCWQ9Nw4brR","executionInfo":{"status":"ok","timestamp":1733673076345,"user_tz":-210,"elapsed":52731,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"25285ccd-01d5-4211-9353-1b6ca0bd5ee1"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Preprocessing audio...\n","Preprocessing completed. File saved as: /content/drive/MyDrive/Speech2Text/dataset//preprocessed.wav\n","Audio duration: 679.00 seconds\n","Chunk 0.00-30.00 transcribed: اونجا بودن\n","Chunk 25.00-55.00 transcribed: زرگ\n","Chunk 50.00-80.00: [Unintelligible]\n","Chunk 75.00-105.00: [Unintelligible]\n","Chunk 100.00-130.00 transcribed: انی بود\n","Chunk 125.00-155.00 transcribed: ن که فوت شدن می‌گفتن سر دیگه آقا مثلاً چشمشون دیگه کور شد انقدر مامان می‌گفتن سر فوت آقا دیگه چشمشون کور شد مامان\n","Chunk 150.00-180.00 transcribed: بچه بزرگ\n","Chunk 175.00-205.00 transcribed: مامان\n","Chunk 200.00-230.00 transcribed: ت\n","Chunk 225.00-255.00 transcribed: یه نفر\n","Chunk 250.00-280.00 transcribed:  بهش تذکر می‌دادن مغرور بودن دخترا\n","Chunk 275.00-305.00 transcribed: \n","Chunk 300.00-330.00 transcribed: دیگه میومدن نمازی تهران بودن\n","Chunk 325.00-355.00 transcribed: م زندگی خیلی اذیت بوده مامانم میگفت\n","Chunk 350.00-380.00 transcribed: عمه\n","Chunk 375.00-405.00 transcribed:  دیسیپلین خودشونو داشتن بعد اینا نه مثلاً اینا اصلاً اینجوری احترام عمه رو نگه نمی‌داشتند مامان می‌گفتن احترامشونو خیلی نگه نمی‌داشتن عروساشون بله عمه مامان جون دیگه خاله\n","Chunk 400.00-430.00 transcribed: ری\n","Chunk 425.00-455.00 transcribed: اطاعت تا کتاب\n","Chunk 450.00-480.00 transcribed: ا\n","Chunk 475.00-505.00 transcribed: ره بودن\n","Chunk 500.00-530.00 transcribed: اشتم\n","Chunk 525.00-555.00 transcribed: ده بودن که\n","Chunk 550.00-580.00 transcribed: کاغذ سفید\n","Chunk 575.00-605.00 transcribed: تاب یه کتاب هست راجع به هستش کتاب\n","Chunk 600.00-630.00 transcribed: آداب معاشرت با زنان منتها نه اینا یه سری خرافات خیلی هم داره\n","Chunk 625.00-655.00 transcribed:  کار نکن شنبه اون کار نکنی از اینجور چیزاست دیگه\n","Chunk 650.00-679.00: [Unintelligible]\n","Chunk 675.00-679.00: [Unintelligible]\n","Transcription saved to /content/drive/MyDrive/Speech2Text/dataset//transcription_output_chunk_30_cover_overlap.txt\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n","import torchaudio\n","from torchaudio.transforms import Resample\n","\n","model_name = \"facebook/s2t-small-librispeech-asr\"\n","processor = Speech2TextProcessor.from_pretrained(model_name)\n","model = Speech2TextForConditionalGeneration.from_pretrained(model_name)\n","model.eval()\n","\n","def preprocess_audio(audio_path, target_sample_rate=16000, max_duration=30):\n","    waveform, sample_rate = torchaudio.load(audio_path)\n","    print(f\"Original waveform shape: {waveform.shape}, Sample rate: {sample_rate}\")\n","\n","    # Resample to target sample rate\n","    if sample_rate != target_sample_rate:\n","        resampler = Resample(orig_freq=sample_rate, new_freq=target_sample_rate)\n","        waveform = resampler(waveform)\n","        print(f\"Waveform resampled to {target_sample_rate} Hz\")\n","\n","    # Convert to mono if stereo\n","    if waveform.shape[0] > 1:\n","        waveform = torch.mean(waveform, dim=0, keepdim=True)\n","        print(\"Converted stereo to mono\")\n","\n","    # Limit waveform to max duration\n","    max_length = target_sample_rate * max_duration\n","    if waveform.shape[1] > max_length:\n","        waveform = waveform[:, :max_length]\n","        print(f\"Trimmed waveform to {max_duration} seconds\")\n","\n","    print(f\"Processed waveform shape: {waveform.shape}\")\n","    return waveform\n","\n","def transcribe_audio(audio_path):\n","    waveform = preprocess_audio(audio_path)\n","\n","    # Normalize audio\n","    waveform = waveform / torch.max(torch.abs(waveform))\n","\n","    inputs = processor(waveform.squeeze(0), sampling_rate=16000, return_tensors=\"pt\")\n","    print(f\"Processor input features shape: {inputs['input_features'].shape}\")\n","\n","    with torch.no_grad():\n","        generated_ids = model.generate(inputs_embeds=inputs[\"input_features\"])\n","\n","    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)\n","    return transcription[0]\n","\n","\n","if __name__ == \"__main__\":\n","    main_path = '/content/drive/MyDrive/Speech2Text/dataset/madar0'\n","    audio_file = f\"{main_path}/Lab41-SRI-VOiCES-madar.wav\"\n","    try:\n","        transcription = transcribe_audio(audio_file)\n","        print(f\"Transcription: {transcription}\")\n","    except Exception as e:\n","        print(f\"Error: {e}\")"],"metadata":{"id":"QDv0P40Wd7DJ","executionInfo":{"status":"ok","timestamp":1733400271478,"user_tz":-210,"elapsed":14,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"dbbb8889-2053-482e-aee7-ac9c54f349da"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of Speech2TextForConditionalGeneration were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["Original waveform shape: torch.Size([2, 10076015]), Sample rate: 44100\n","Waveform resampled to 16000 Hz\n","Converted stereo to mono\n","Trimmed waveform to 30 seconds\n","Processed waveform shape: torch.Size([1, 480000])\n","Processor input features shape: torch.Size([1, 2998, 80])\n","Error: Dimension out of range (expected to be in range of [-2, 1], but got 2)\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n","from datasets import load_dataset\n","\n","model = Speech2TextForConditionalGeneration.from_pretrained(\"facebook/s2t-medium-mustc-multilingual-st\")\n","processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-medium-mustc-multilingual-st\")\n","\n","ds = load_dataset(\"hf-internal-testing/librispeech_asr_demo\", \"clean\", split=\"validation\")\n","\n","inputs = processor(ds[0][\"audio\"][\"array\"], sampling_rate=ds[0][\"audio\"][\"sampling_rate\"], return_tensors=\"pt\")\n","generated_ids = model.generate(\n","    inputs[\"input_features\"],\n","    attention_mask=inputs[\"attention_mask\"],\n","    forced_bos_token_id=processor.tokenizer.lang_code_to_id[\"fr\"],\n",")\n","\n","translation = processor.batch_decode(generated_ids, skip_special_tokens=True)\n","translation"],"metadata":{"id":"5_WtJD5pXFZt","executionInfo":{"status":"ok","timestamp":1733400274952,"user_tz":-210,"elapsed":3481,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"e1afc0ed-61ec-41bc-ab22-4ec8236ad26c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of Speech2TextForConditionalGeneration were not initialized from the model checkpoint at facebook/s2t-medium-mustc-multilingual-st and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["[\"(Vidéo) Si M. Kilder est l'apossible des classes moyennes, et nous sommes heureux d'être accueillis dans son évangile.\"]"]},"metadata":{},"execution_count":52}]},{"cell_type":"code","source":["from transformers import Speech2TextConfig, Speech2TextModel\n","\n","configuration = Speech2TextConfig()\n","\n","model = Speech2TextModel(configuration)\n","\n","configuration = model.config"],"metadata":{"id":"sD-1lyNxXGhl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n","import soundfile as sf\n","import torch\n","\n","model_name = \"facebook/s2t-small-librispeech-asr\"\n","model = Speech2TextForConditionalGeneration.from_pretrained(model_name)\n","processor = Speech2TextProcessor.from_pretrained(model_name)"],"metadata":{"id":"iCsEZbVVpbdW","executionInfo":{"status":"ok","timestamp":1733400278992,"user_tz":-210,"elapsed":4043,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d39cb949-7724-4c9b-821d-befd341eafa7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of Speech2TextForConditionalGeneration were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","source":["# import wave\n","# w = wave.open('Lab41-SRI-VOiCES-src-sp0307-ch127535-sg0042.wav', 'r')\n","# for i in range(w.getnframes()):\n","#     SPEECH_FILE = w.readframes(i)\n","#     # print(SPEECH_FILE)\n","# print(SPEECH_FILE)"],"metadata":{"id":"eebiWaP9YGco"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import IPython\n","\n","# IPython.display.Audio(SPEECH_FILE)"],"metadata":{"id":"220-PTPJqF-t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import soundfile as sf\n","\n","def load_audio(file_path):\n","    speech, sampling_rate = sf.read(file_path)\n","    return {\"speech\": speech, \"sampling_rate\": sampling_rate}\n","\n","main_path = '/content/drive/MyDrive/Speech2Text/dataset/madar0'\n","audio_data = load_audio(f'{main_path}/Lab41-SRI-VOiCES-madar.wav')\n","print(f\"Original sampling rate: {audio_data['sampling_rate']}\")"],"metadata":{"id":"EB755pLEq3bP","executionInfo":{"status":"ok","timestamp":1733400278993,"user_tz":-210,"elapsed":13,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"d7b9f3f1-962a-4206-ca9c-9d1389c81c60"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Original sampling rate: 44100\n"]}]},{"cell_type":"code","source":["import torch\n","print(torch.cuda.is_available())\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)"],"metadata":{"id":"vkScF1jr9A46","executionInfo":{"status":"ok","timestamp":1733400278993,"user_tz":-210,"elapsed":11,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"842baabd-5e08-4447-ece2-60a779b3e7cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["False\n"]},{"output_type":"execute_result","data":{"text/plain":["Speech2TextForConditionalGeneration(\n","  (model): Speech2TextModel(\n","    (encoder): Speech2TextEncoder(\n","      (conv): Conv1dSubsampler(\n","        (conv_layers): ModuleList(\n","          (0): Conv1d(80, 1024, kernel_size=(5,), stride=(2,), padding=(2,))\n","          (1): Conv1d(512, 512, kernel_size=(5,), stride=(2,), padding=(2,))\n","        )\n","      )\n","      (embed_positions): Speech2TextSinusoidalPositionalEmbedding()\n","      (layers): ModuleList(\n","        (0-11): 12 x Speech2TextEncoderLayer(\n","          (self_attn): Speech2TextAttention(\n","            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","          )\n","          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (activation_fn): ReLU()\n","          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n","          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (decoder): Speech2TextDecoder(\n","      (embed_tokens): Embedding(10000, 256, padding_idx=1)\n","      (embed_positions): Speech2TextSinusoidalPositionalEmbedding()\n","      (layers): ModuleList(\n","        (0-5): 6 x Speech2TextDecoderLayer(\n","          (self_attn): Speech2TextAttention(\n","            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","          )\n","          (activation_fn): ReLU()\n","          (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (encoder_attn): Speech2TextAttention(\n","            (k_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (v_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (q_proj): Linear(in_features=256, out_features=256, bias=True)\n","            (out_proj): Linear(in_features=256, out_features=256, bias=True)\n","          )\n","          (encoder_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","          (fc1): Linear(in_features=256, out_features=2048, bias=True)\n","          (fc2): Linear(in_features=2048, out_features=256, bias=True)\n","          (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","      (layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n","    )\n","  )\n","  (lm_head): Linear(in_features=256, out_features=10000, bias=False)\n",")"]},"metadata":{},"execution_count":58}]},{"cell_type":"code","source":["import librosa\n","\n","def resample_audio(audio_data):\n","    speech = audio_data[\"speech\"]\n","    sampling_rate = audio_data[\"sampling_rate\"]\n","    if sampling_rate != 16000:\n","        speech = librosa.resample(speech, orig_sr=sampling_rate, target_sr=16000)\n","    return {\"speech\": speech, \"sampling_rate\": 16000}\n","\n","# from scipy.signal import resample\n","\n","# def resample_audio(audio_data):\n","#     speech = audio_data[\"speech\"]\n","#     sampling_rate = audio_data[\"sampling_rate\"]\n","#     if sampling_rate != 16000:\n","#         num_samples = int(len(speech) * 16000 / sampling_rate)\n","#         speech = resample(speech, num_samples)\n","#     return {\"speech\": speech, \"sampling_rate\": 16000}\n","\n","# resampled_audio = resample_audio(audio_data)\n","# print(f\"Resampled sampling rate: {resampled_audio['sampling_rate']}\")"],"metadata":{"id":"59_5laroq7rd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n","import torch\n","from IPython.display import Audio\n","\n","def preprocess_audio(file_path):\n","    # audio_data = load_audio(file_path)\n","\n","    audio_data, rate = librosa.load(file_path, sr=16000)\n","    Audio(audio_data, rate=rate)\n","    # resampled_audio = resample_audio(audio_data)\n","\n","    processor = Speech2TextProcessor.from_pretrained(\"facebook/s2t-small-librispeech-asr\")\n","    inputs = processor(resampled_audio[\"speech\"], sampling_rate=resampled_audio[\"sampling_rate\"], return_tensors=\"pt\")\n","    return inputs\n","\n","model_name = \"facebook/s2t-small-librispeech-asr\"\n","model = Speech2TextForConditionalGeneration.from_pretrained(model_name)\n","\n","main_path = '/content/drive/MyDrive/Speech2Text/dataset/madar0'\n","inputs = preprocess_audio(f'{main_path}/Lab41-SRI-VOiCES-madar.wav')\n","with torch.no_grad():\n","    logits = model(**inputs).logits\n","\n","processor = Speech2TextProcessor.from_pretrained(model_name)\n","transcription = processor.batch_decode(logits, skip_special_tokens=True)[0]\n","print(f\"Transcription: {transcription}\")"],"metadata":{"id":"uhlMSmG4pfQP","executionInfo":{"status":"error","timestamp":1733405663729,"user_tz":-210,"elapsed":3773,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"58aca74e-dafe-4db9-c877-732ae73407cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of Speech2TextForConditionalGeneration were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"error","ename":"IndexError","evalue":"only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-62-7bf676e0a251>\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mmain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/content/drive/MyDrive/Speech2Text/dataset/madar0'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{main_path}/Lab41-SRI-VOiCES-madar.wav'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-62-7bf676e0a251>\u001b[0m in \u001b[0;36mpreprocess_audio\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0maudio_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mresampled_audio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresample_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprocessor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSpeech2TextProcessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"facebook/s2t-small-librispeech-asr\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-61-c43adb427837>\u001b[0m in \u001b[0;36mresample_audio\u001b[0;34m(audio_data)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mresample_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mspeech\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"speech\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0msampling_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maudio_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"sampling_rate\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0msampling_rate\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m16000\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: only integers, slices (`:`), ellipsis (`...`), numpy.newaxis (`None`) and integer or boolean arrays are valid indices"]}]},{"cell_type":"code","source":["from transformers import Speech2TextProcessor, Speech2TextForConditionalGeneration\n","import torch\n","import librosa\n","\n","model_name = \"facebook/s2t-small-librispeech-asr\"\n","model = Speech2TextForConditionalGeneration.from_pretrained(model_name)\n","processor = Speech2TextProcessor.from_pretrained(model_name)\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model.to(device)\n","\n","def load_audio(file_path):\n","    speech, sampling_rate = librosa.load(file_path, sr=None)\n","    return {\"speech\": speech, \"sampling_rate\": sampling_rate}\n","\n","def resample_audio(audio_data):\n","    speech = audio_data[\"speech\"]\n","    sampling_rate = audio_data[\"sampling_rate\"]\n","    if sampling_rate != 16000:\n","        speech = librosa.resample(speech, orig_sr=sampling_rate, target_sr=16000)\n","    return {\"speech\": speech, \"sampling_rate\": 16000}\n","\n","def transcribe_audio(file_path):\n","    audio_data = load_audio(file_path)\n","    resampled_audio = resample_audio(audio_data)\n","\n","    inputs = processor(resampled_audio[\"speech\"], sampling_rate=resampled_audio[\"sampling_rate\"], return_tensors=\"pt\", padding=True)\n","    inputs = {k: v.to(device) for k, v in inputs.items()}\n","\n","    with torch.no_grad():\n","        logits = model(**inputs).logits\n","\n","    transcription = processor.batch_decode(logits, skip_special_tokens=True)[0]\n","    return transcription\n","\n","main_path = '/content/drive/MyDrive/Speech2Text/dataset/madar0'\n","audio_file = f\"{main_path}/output.wav\"\n","input_path = f'{main_path}/Lab41-SRI-VOiCES-madar.wav'\n","transcription = transcribe_audio(input_path)\n","print(transcription)"],"metadata":{"id":"OUgIT3BwoTqA","executionInfo":{"status":"error","timestamp":1733404608501,"user_tz":-210,"elapsed":45372,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"colab":{"base_uri":"https://localhost:8080/","height":465},"outputId":"133ab0ed-9eb6-4da7-9527-5d4eebcde141"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of Speech2TextForConditionalGeneration were not initialized from the model checkpoint at facebook/s2t-small-librispeech-asr and are newly initialized: ['model.decoder.embed_positions.weights', 'model.encoder.embed_positions.weights']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"error","ename":"ValueError","evalue":"You have to specify either decoder_input_ids or decoder_inputs_embeds","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-61-c43adb427837>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0maudio_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{main_path}/output.wav\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0minput_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{main_path}/Lab41-SRI-VOiCES-madar.wav'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mtranscription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranscribe_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranscription\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-61-c43adb427837>\u001b[0m in \u001b[0;36mtranscribe_audio\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mtranscription\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/speech_to_text/modeling_speech_to_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                 )\n\u001b[1;32m   1294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1295\u001b[0;31m         outputs = self.model(\n\u001b[0m\u001b[1;32m   1296\u001b[0m             \u001b[0minput_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/speech_to_text/modeling_speech_to_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_features, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1177\u001b[0;31m         decoder_outputs = self.decoder(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_input_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoder_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/speech_to_text/modeling_speech_to_text.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    959\u001b[0m             \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    960\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 961\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You have to specify either decoder_input_ids or decoder_inputs_embeds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    962\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    963\u001b[0m         \u001b[0;31m# past_key_values_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: You have to specify either decoder_input_ids or decoder_inputs_embeds"]}]},{"cell_type":"code","source":[],"metadata":{"id":"CD-49g4oonHz"},"execution_count":null,"outputs":[]}]}