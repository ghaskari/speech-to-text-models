{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"SdqM73Ibh8Mp"},"source":["# mp3 to text with deep speech model\n","\n","---\n","\n","![speech to text](https://uploads-ssl.webflow.com/5985ca0c9abf440001d1f4b0/5a68a52180efb200017181cf_transcription_icon_v2_EN.png)"]},{"cell_type":"code","source":["!pip3 install deepspeech"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u7l88JY2zuxV","executionInfo":{"status":"ok","timestamp":1733670502071,"user_tz":-210,"elapsed":1115,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}},"outputId":"ad4032c7-fc85-4591-801c-6750ac2244b5"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[31mERROR: Could not find a version that satisfies the requirement deepspeech (from versions: none)\u001b[0m\u001b[31m\n","\u001b[0m\u001b[31mERROR: No matching distribution found for deepspeech\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","metadata":{"id":"CrH9g0n1hzpK","outputId":"27efec18-c055-4ab8-d028-f232be27f239","colab":{"base_uri":"https://localhost:8080/","height":401},"executionInfo":{"status":"error","timestamp":1733670444722,"user_tz":-210,"elapsed":5,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}}},"source":["from __future__ import absolute_import, division, print_function\n","\n","import os\n","import numpy as np\n","import shlex\n","import subprocess\n","import sys\n","import wave\n","\n","from deepspeech import Model, printVersions\n","from timeit import default_timer as timer\n","\n","# audio converters\n","!apt update && apt-get install ffmpeg mpg123\n","\n","# sox package for adjusting sample rate.\n","!apt-get install libsox-fmt-all libsox-dev sox\n","\n","# neural network model for acoustic recognition\n","!wget -O - https://github.com/mozilla/DeepSpeech/releases/download/v0.3.0/deepspeech-0.3.0-models.tar.gz | tar xvfz -"],"execution_count":6,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"No module named 'deepspeech'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-204b1f205fa8>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwave\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdeepspeech\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprintVersions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtimeit\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefault_timer\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'deepspeech'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"],"errorDetails":{"actions":[{"action":"open_url","actionText":"Open Examples","url":"/notebooks/snippets/importing_libraries.ipynb"}]}}]},{"cell_type":"markdown","metadata":{"id":"fr-SPvuT_nEF"},"source":["# Accoustic parameters"]},{"cell_type":"code","metadata":{"id":"lQFywq8DlIbv","executionInfo":{"status":"aborted","timestamp":1733670292906,"user_tz":-210,"elapsed":10,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}}},"source":["model    = 'models/output_graph.pbmm'\n","alphabet = 'models/alphabet.txt'\n","lm       = 'models/lm.binary'\n","trie     = 'models/trie'\n","\n","# These constants control the beam search decoder\n","\n","# Beam width used in the CTC decoder when building candidate transcriptions\n","BEAM_WIDTH = 500\n","\n","# The alpha hyperparameter of the CTC decoder. Language Model weight\n","LM_WEIGHT = 1.50\n","\n","# Valid word insertion weight. This is used to lessen the word insertion penalty\n","# when the inserted word is part of the vocabulary\n","VALID_WORD_COUNT_WEIGHT = 2.10\n","\n","\n","# These constants are tied to the shape of the graph used (changing them changes\n","# the geometry of the first layer), so make sure you use the same constants that\n","# were used during training\n","\n","# Number of MFCC features to use\n","N_FEATURES = 26\n","\n","# Size of the context window used for producing timesteps in the input vector\n","N_CONTEXT = 9"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QDlyOTMU_um-"},"source":["# Adapt Sample Rate of Audio File"]},{"cell_type":"code","metadata":{"id":"bC5kgGTzlNop","executionInfo":{"status":"aborted","timestamp":1733670292906,"user_tz":-210,"elapsed":9,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}}},"source":["def convert_samplerate(audio_path):\n","    sox_cmd = 'sox {} --type raw --bits 16 --channels 1 --rate 16000 --encoding signed-integer --endian little --compression 0.0 --no-dither - '.format(audio_path)\n","    try:\n","        output = subprocess.check_output(shlex.split(sox_cmd), stderr=subprocess.PIPE)\n","    except subprocess.CalledProcessError as e:\n","        raise RuntimeError('SoX returned non-zero status: {}'.format(e.stderr))\n","    except OSError as e:\n","        raise OSError(e.errno, 'SoX not found, use 16kHz files or install it: {}'.format(e.strerror))\n","\n","    return 16000, np.frombuffer(output, np.int16)\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"S6WuuR5S_5tu"},"source":["# Input MP3 Audio File"]},{"cell_type":"code","metadata":{"id":"byQfXtXLqD-A","executionInfo":{"status":"aborted","timestamp":1733670292906,"user_tz":-210,"elapsed":9,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}}},"source":["# upload mp3 audio file.\n","\n","from google.colab import files\n","uploaded = files.upload()\n","for audio in uploaded.keys():\n","  print('User uploaded file \"{name}\" with length {length} bytes'.format(\n","         name=audio, length=len(uploaded[audio])))\n","\n","os.rename(audio, 'speech.mp3')\n","audio = 'speech.wav'\n","\n","# convert to wav file.\n","!ffmpeg -i speech.mp3 -vn -acodec pcm_s16le -ac 1 -ar 16000 -f wav speech.wav\n","#!mpg123 -w speech.wav speech.mp3"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kI1ufWC4AAsq"},"source":["# Convert MP3 to Text"]},{"cell_type":"code","metadata":{"id":"aubQx7CilQGF","executionInfo":{"status":"aborted","timestamp":1733670292907,"user_tz":-210,"elapsed":10,"user":{"displayName":"Ghazal Askari","userId":"11802624708929709832"}}},"source":["    print('Loading model from file {}'.format(model), file=sys.stderr)\n","    model_load_start = timer()\n","    ds = Model(model, N_FEATURES, N_CONTEXT, alphabet, BEAM_WIDTH)\n","    model_load_end = timer() - model_load_start\n","    print('Loaded model in {:.3}s.'.format(model_load_end), file=sys.stderr)\n","\n","    if lm and trie:\n","        print('Loading language model from files {} {}'.format(lm, trie), file=sys.stderr)\n","        lm_load_start = timer()\n","        ds.enableDecoderWithLM(alphabet, lm, trie, LM_WEIGHT,\n","                               VALID_WORD_COUNT_WEIGHT)\n","        lm_load_end = timer() - lm_load_start\n","        print('Loaded language model in {:.3}s.'.format(lm_load_end), file=sys.stderr)\n","\n","    fin = wave.open(audio, 'rb')\n","    fs = fin.getframerate()\n","    if fs != 16000:\n","        print('Warning: original sample rate ({}) is different than 16kHz. Resampling might produce erratic speech recognition.'.format(fs), file=sys.stderr)\n","        fs, audio = convert_samplerate(audio)\n","    else:\n","        audio = np.frombuffer(fin.readframes(fin.getnframes()), np.int16)\n","\n","    audio_length = fin.getnframes() * (1/16000)\n","    fin.close()\n","\n","    print('Running inference.', file=sys.stderr)\n","    print('================================\\n')\n","    inference_start = timer()\n","    print(ds.stt(audio, fs))\n","    inference_end = timer() - inference_start\n","    print('\\n================================')\n","    print('Inference took %0.3fs for %0.3fs audio file.' % (inference_end, audio_length), file=sys.stderr)"],"execution_count":null,"outputs":[]}]}